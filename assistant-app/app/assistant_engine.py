import openai
import vosk
import datetime
import json
import os
import config
import re

# ğŸ§  Greek phonetic corrections for technical terms
PHONETIC_GREEK_CORRECTIONS = {
    "f1 ÏƒÎºÎ¿Ï": "f1 score",
    "Î¬Î½Ï„Î±Î¼ ÏŒÏ€Ï„Î¹Î¼Î±ÏŠÎ¶ÎµÏ": "adam optimizer",
    "Î¬Î½Ï„ÎµÏÏ†Î¹Ï„Î¹Î½Î³Îº": "underfitting",
    "Î¬Î½Ï„Î¹ Î¼Î¿Î½ÏŒÏ„Î¿Î½Î¹Îº Ï€ÏÏŒÏ€ÎµÏÏ„Î¹": "anti-monotonic property",
    "Î¬Î¿Ï…Ï„Î»Î¬Î¹ÎµÏ Î½Ï„ÎµÏ„Î­Î¾Î¹Î¿Î½": "outlier detection",
    "Î¬Ï€ÏÎ¹Î¿ÏÎ¹": "apriori",
    "Î­Î½Ï„ÏÎ¿Ï€Î¹": "entropy",
    "Î±Î³ÎºÎ»Î¿Î¼ÎµÏÎ±Ï„Î¹Î²": "agglomerative",
    "Î±ÎºÎºÎ¹Î¿ÏÏÎ±ÏƒÎ·": "accuracy",
    "Î±ÎºÏ„Î¹Î²Î­Î¹ÏƒÎ¹Î¿Î½ Ï†Î¬Î½ÎºÏƒÎ¹Î¿Î½": "activation function",
    "Î±Î½Ï„Î¶Î¬ÏƒÏ„ÎµÎ½Ï„ Î±Ï ÏƒÎºÎ¿Ï…Î­ÏÎ½Ï„": "adjusted r-squared",
    "Î±Î¿Ï…Îº": "auc",
    "Î±Ï": "r",
    "Î±Ï ÎµÎ½ ÎµÎ½": "rnn",
    "Î±Ï Î¼Î¬ÏÎºÎ½Ï„Î±Î¿Ï…Î½": "rmarkdown",
    "Î±Ï ÏƒÎºÎ¿Ï…Î­ÏÎ½Ï„": "r-squared",
    "Î±ÏÎ¯Î± Î±Î½Ï„ÎµÏ Î´Îµ ÎºÎµÏÎ²": "area under the curve",
    "Î±ÏÎ¯Î¼Î±": "arima",
    "Î±ÏƒÎ¿ÏƒÎ¹Î­Î¹ÏƒÎ¹Î¿Î½ ÏÎ¿Ï…Î»": "association rule",
    "Î²Î¬Î»Î¹Î½Ï„Î­Î¹ÏƒÎ¹Î¿Î½": "validation set",
    "Î²Î­ÏÎ¹Î±Î½Ï‚": "variance",
    "Î³ÎºÎ¬Î¿Ï…ÏƒÎ¹Î±Î½ Î¼Î¯Î¾Ï„Î¿Ï…Ï Î¼ÏŒÎ´ÎµÎ»": "gaussian mixture model",
    "Î³ÎºÏÎµÎ½Ï„Î¹ÎµÎ½Ï„ Î½Ï„Î¹ÏƒÏƒÎ­Î½Ï„": "gradient descent",
    "Î³ÎºÏÎ¹Î½Ï„ ÏƒÎµÏÏ„Ï‚": "grid search",
    "Î³Î¿Ï…Î­ÏÎºÏ†Î»Î¿Î¿Ï…": "workflow",
    "Î³Î¿Ï…ÏŒÏÎ½Ï„ Ï„Î¿Ï… Î²ÎµÎº": "word2vec",
    "ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·": "training",
    "ÎµÎ» ÎµÏ‚ Ï„Î¹ Î­Î¼": "lstm",
    "ÎµÎ» Î½Ï„Î¹ Î­Î¹": "lda",
    "ÎµÎ» Î¿Ï…Î¬Î½": "l1",
    "ÎµÎ» Ï„Î¿Ï…": "l2",
    "ÎµÎ¼ Î­Î»": "ML",
    "ÎµÎ¼ ÎµÏ‚ Î¹": "mse",
    "ÎµÎ½ Î­Î» Ï€Î¹": "nlp",
    "ÎµÎ½ÏƒÎ±Î¼Ï€Î»": "ensemble",
    "ÎµÎ¾ Ï„Î¶Î¹ Î¼Ï€Î¿ÏÏƒÏ„": "xgboost",
    "ÎµÏ€Î¿Ï‡Î®": "epoch",
    "ÎµÏÏÏ Ï†Î¬Î½ÎºÏƒÎ¹Î¿Î½": "error function",
    "ÎµÏ† Î­Î½Î± ÏƒÎºÎ¿Ï": "f1-score",
    "ÎµÏŠ Î±ÏŠ": "AI",
    "Î¶Î® ÏƒÎºÎ¿Ï": "z-score",
    "Î¶Î® ÏƒÎºÎ¿Ï Î½Î¿ÏÎ¼Î±Î»Î±ÏŠÎ¶Î­Î¹ÏƒÎ¹Î¿Î½": "z-score normalization",
    "Î¹ ÎµÎ½ Ï‡Î¯Î»Î¹Î± ÎµÎ²Î´Î¿Î¼Î®Î½Ï„Î± Î­Î½Î±": "e1071",
    "Î¹Î¼Ï€Î¹Î¿Ï…Ï„Î­Î¹ÏƒÎ¹Î¿Î½": "imputation",
    "Î¹Î½Ï†Î¿ÏÎ¼Î­Î¹ÏƒÎ¹Î¿Î½ Î³ÎºÎ­Î¹Î½": "information gain",
    "ÎºÎ¬Î¹ ÏƒÎºÎ¿Ï…Î­Ï Ï„ÎµÏƒÏ„": "chi-square test",
    "ÎºÎ¬ÏÎµÏ„": "caret",
    "ÎºÎ­ÏÎ±Ï‚": "keras",
    "ÎºÎ­ÏÏ‚ Î¿Ï† Î½Ï„Î±ÏŠÎ¼ÎµÎ½ÏƒÎ¹Î¿Î½Î¬Î»Î¹Ï„Î¹": "curse of dimensionality",
    "ÎºÎ±Î½": "knn",
    "ÎºÎ­Î¹ ÎµÎ½ ÎµÎ½": "knn",
    "ÎºÎ±Ï„ Î¼Ï€Î¿ÏÏƒÏ„": "catboost",
    "ÎºÎµÏÏ„ÏŒÏƒÎ¹Ï‚": "kurtosis",
    "ÎºÎµÏŠ Î¼Î·Î½Ï‚": "k-means",
    "ÎºÎ»Î¬ÏƒÎ¹Ï†Î¹ÎºÎ­Î¹ÏƒÎ¹Î¿Î½": "classification",
    "ÎºÎ»Î¬ÏƒÏ„ÎµÏÎ¹Î½Î³Îº": "clustering",
    "ÎºÎ¿Î½Ï„Î¯ÏƒÎ¹Î¿Î½Î±Î» Ï€ÏÎ¿Î¼Ï€Î±Î¼Ï€Î¯Î»Î¹Ï„Î¹": "conditional probability",
    "ÎºÎ¿Î½Ï†Î¬Î¿Ï…Î½Ï„Î¹Î½Î³Îº Î²Î±ÏÎ¹Î±Î¼Ï€Î»": "confounding variable",
    "ÎºÎ¿Î½Ï†Î¹Î¿Ï…Î¶Î¿Î½ Î¼Î¬Ï„ÏÎ¹Î¾": "confusion matrix",
    "ÎºÏÎ¿Ï‚ Î­Î½Ï„ÏÎ¿Ï€Î¹": "cross-entropy",
    "ÎºÏÎ¿Ï‚ Î²Î±Î»Î¹Î½Ï„Î­Î¹ÏƒÎ¹Î¿Î½": "cross-validation",
    "ÎºÏŒÎ½Ï†Î¹Î½Ï„ÎµÎ½Ï‚": "confidence",
    "ÎºÏŒÎ½Ï†Î¹Î½Ï„ÎµÎ½Ï‚ Î¯Î½Ï„ÎµÏÎ²Î±Î»": "confidence interval",
    "Î»Î¬Î¹ÎºÎ»Î¹Ï‡Î¿Ï…Î½Ï„": "likelihood",
    "Î»Î¬Î¹Ï„ Ï„Î¶Î¹ Î¼Ï€Î¹ Î­Î¼": "lightgbm",
    "Î»Î¬ÏƒÏƒÎ¿": "lasso",
    "Î»Î­Î¹Î¼Ï€ÎµÎ» ÎµÎ½ÎºÏŒÎ½Ï„Î¹Î½Î³Îº": "label encoding",
    "Î»Î­ÏÎ½Î¹Î½Î³Îº ÏÎ­Î¹Ï„": "learning rate",
    "Î»Î±Î³Îº": "lag",
    "Î»ÎµÎ¼Î¼Î±Ï„Î¹Î¶Î­Î¹ÏƒÎ¹Î¿Î½": "lemmatization",
    "Î»Î¹Î½ÎµÎ±Ï ÏÎµÎ³ÎºÏÎ­ÏƒÎ¹Î¿Î½": "linear regression",
    "Î»Î¹Ï†Ï„": "lift",
    "Î»Î¿Î³ Î»Î¿Ï‚": "log loss",
    "Î»Î¿Ï„Î¶Î¯ÏƒÏ„Î¹Îº ÏÎµÎ³ÎºÏÎ­ÏƒÎ¹Î¿Î½": "logistic regression",
    "Î»ÏŒÏƒ Ï†Î¬Î½ÎºÏƒÎ¹Î¿Î½": "loss function",
    "Î¼Î¬Î¾Î¹Î¼Î¿Ï…Î¼ Î»Î¬Î¹ÎºÎ»Î¹Ï‡Î¿Ï…Î½Ï„ ÎµÏƒÏ„Î¹Î¼Î­Î¹ÏƒÎ¹Î¿Î½": "maximum likelihood estimation",
    "Î¼Î¬Ï„ÏÎ¹Î¾": "matrix",
    "Î¼Î®Î½Ï„Î¹Î±Î½": "median",
    "Î¼Î¯ÏƒÏƒÎ¹Î½Î³Îº Î²Î¬Î»Î¹Î¿Ï…Î¶": "missing values",
    "Î¼Î±ÏƒÎ¯Î½ Î»Î­ÏÎ½Î¹Î½Î³Îº": "machine learning",
    "Î¼Î±Ï„Ï€Î»ÏŒÏ„Î»Î¹Î¼Ï€": "matplotlib",
    "Î¼Î¹Î¼ Î¼Î±Î¾ ÏƒÎºÎ­Î¹Î»Î¹Î½Î³Îº": "min-max scaling",
    "Î¼Î¹Î½": "mean",
    "Î¼Î¹Î½ Î¬Î¼Ï€ÏƒÎ¿Î»Î¿Ï…Ï„ Î­ÏÎ¿Ï": "mean absolute error",
    "Î¼Î¹Î½ ÏƒÎ¹Ï†Ï„": "mean shift",
    "Î¼Î¹Î½ ÏƒÎºÎ¿Ï…Î­ÏÎ½Ï„ Î­ÏÎ¿Ï": "mean squared error",
    "Î¼Ï€Î¬Î³ÎºÎ¹Î½Î³Îº": "bagging",
    "Î¼Ï€Î¬Î¹Î±Ï‚": "bias",
    "Î¼Ï€Î­Î¹Î¶ Î¸Î¯Î¿ÏÎµÎ¼": "bayes theorem",
    "Î¼Ï€Î­Î¹Î¶Î¹Î±Î½ Î¯Î½Ï†ÎµÏÎµÎ½Ï‚": "bayesian inference",
    "Î¼Ï€Î±ÎºÏ€ÏÎ¿Ï€Î±Î³ÎºÎ­Î¹ÏƒÎ¹Î¿Î½": "backpropagation",
    "Î¼Ï€ÎµÏÏ„": "bert",
    "Î¼Ï€Î¹Î³Îº Î½Ï„Î­Î¹Ï„Î±": "big data",
    "Î¼Ï€Î¿ÏÏƒÏ„Î¹Î½Î³Îº": "boosting",
    "Î¼ÏŒÎ½Ï„ÎµÎ» Î½Ï„Î¹Ï€Î»ÏŒÎ¹Î¼ÎµÎ½Ï„": "model deployment",
    "Î¼ÏŒÎ¿Ï…Î½Ï„": "mode",
    "Î½Î±Î» Ï‡Î±ÏŠÏ€ÏŒÎ¸ÎµÏƒÎ¹Ï‚": "null hypothesis",
    "Î½Î±ÏŠÎ² Î¼Ï€Î­Î¹Ï‚": "naive bayes",
    "Î½ÎµÎ¹Î¼Ï€Î¿ÏÏ‚":"neighbours",
    "Î½ÎµÏ…ÏÏ‰Î½Î¹ÎºÎ¬ Î´Î¯ÎºÏ„Ï…Î±": "neural networks",
    "Î½Î¹Î¿ÏÏÎ±Î» Î½Î­Ï„Î³Î¿Ï…ÎµÏÎº": "neural network",
    "Î½Î¿ÏÎ¼Î±Î»Î±ÏŠÎ¶Î­Î¹ÏƒÎ¹Î¿Î½": "normalization",
    "Î½Î¿ÏÎ¼Ï€Î±ÏŠ": "numpy",
    "Î½Ï„Î­Î¹Ï„Î± Î²Î¹Î¶Î¿Ï…Î±Î»Î±Î¹Î¶Î­Î¹ÏƒÎ¹Î¿Î½": "data visualization",
    "Î½Ï„Î­Î¹Ï„Î± ÎºÎ»Î¯Î½Î¹Î½Î³Îº": "data cleaning",
    "Î½Ï„Î­Î¹Ï„Î± Ï€Î¬Î¹Ï€Î»Î¬Î¹Î½": "data pipeline",
    "Î½Ï„Î­Î¹Ï„Î± ÏÎ¬Î½Î³ÎºÎ»Î¹Î½Î³Îº": "data wrangling",
    "Î½Ï„Î­Î¹Ï„Î± ÏƒÎ¬Î¹ÎµÎ½Ï‚": "data science",
    "Î½Ï„Î­Î½Î´ÏÎ¿Î³ÎºÏÎ±Î¼": "dendrogram",
    "Î½Ï„Î¹ Î­Î»": "DL",
    "Î½Ï„Î¹ Î¼Ï€Î¹ ÏƒÎºÎ±Î½": "DBSCAN",
    "Î½Ï„Î¹ Ï€Î¹ ÎµÎ» Î³Î¿Ï…Î¬Î±Ï": "dplyr",
    "Î½Ï„Î¹Ï€ Î»Î­ÏÎ½Î¹Î½Î³Îº": "deep learning",
    "Î½Ï„Î¹ÏƒÎ¹Î¶Î¹Î¿Î½ Ï„ÏÎ¹": "decision tree",
    "Î½Ï„ÏÏŒÏ€Î±Î¿Ï…Ï„": "dropout",
    "Î¿Ï„Î¿ÎºÎ¿ÏÎµÎ»Î­Î¹ÏƒÎ¹Î¿Î½": "autocorrelation",
    "Î¿Ï…Î±Î½ Ï‡Î¿Ï„ ÎµÎ½ÎºÏŒÎ½Ï„Î¹Î½Î³Îº": "one-hot encoding",
    "Ï€Î¬Î¹Ï€Î»Î¬Î¹Î½": "pipeline",
    "Ï€Î¬Î¹Ï„Î¿ÏÏ„Ï‚": "pytorch",
    "Ï€Î¬Î½Ï„Î±Ï‚": "pandas",
    "Ï€Î¹ Î²Î¬Î»Î»Î¹Î¿Ï…": "p-value",
    "Ï€Î¹ ÏƒÎ¹ Îµ": "pca",
    "Ï€Î¿ÏƒÏ„Î­ÏÎ¹Î±Ï": "posterior",
    "Ï€ÏÎ¬Î¹Î¿Ï": "prior",
    "Ï€ÏÎ¯Î½ÏƒÎ¹Ï€Î±Î» ÎºÎ¿Î¼Ï€ÏŒÎ¿Ï…Î½ÎµÎ½Ï„ Î±Î½Î±Î»ÏÏƒÎ¹Ï‚": "principal component analysis",
    "Ï€ÏÎµÏƒÎ¯Î¶Î¹Î¿Î½": "precision",
    "Ï€ÏÎ¿Î¼Ï€Î±Î¼Ï€Î¯Î»Î¹Ï„Ï…": "probability",
    "ÏÎ¬Î½Ï„Î¿Î¼ ÏƒÎµÏÏ„Ï‚": "random search",
    "ÏÎ­Î³ÎºÎ¹Î¿Ï…Î»Î±ÏÎ¹Î¶Î­Î¹ÏƒÎ¹Î¿Î½": "regularization",
    "ÏÎ­Î»Î¿Ï…": "relu",
    "ÏÎµÎ³ÎºÏÎ­ÏƒÎ¹Î¿Î½": "regression",
    "ÏÎ¹ÎºÏŒÎ»": "recall",
    "ÏÎ¹Ï„Î¶": "ridge",
    "ÏÎ¿Îº ÎºÎµÏÎ²": "roc curve",
    "ÏÏŒÎ»Î»Î¹Î½Î³Îº Î¬Î²ÎµÏÎ¹Ï„Î¶": "rolling average",
    "ÏƒÎ¬Î¹ÎºÎ¹Ï„ Î»ÎµÏÎ½": "scikit-learn",
    "ÏƒÎ¬Î¼Ï€Î»Î¹Î½Î³Îº Î½Ï„Î¹ÏƒÏ„ÏÎ¹Î¼Ï€Î¹Î¿ÏÏƒÎ¹Î¿Î½": "sampling distribution",
    "ÏƒÎ­Î½Ï„ÏÎ±Î» Î»Î¯Î¼Î¹Ï„ Î¸Î¯Î¿ÏÎµÎ¼": "central limit theorem",
    "ÏƒÎ¯Î³ÎºÎ¼Î¿Î¹Î½Ï„": "sigmoid",
    "ÏƒÎ¯Î¼Ï€Î¿ÏÎ½": "seaborn",
    "ÏƒÎ²Î¼": "support vector machine",
    "ÏƒÎµÎ½ÏƒÎ¹Ï„Î¯Î²Î¹Ï„Î¹": "sensitivity",
    "ÏƒÎ¹ ÎµÎ½ ÎµÎ½": "cnn",
    "ÏƒÎ¹Î¶Î¿Î½Î¬Î»Î¹Ï„Î¹": "seasonality",
    "ÏƒÎ¹Î»Î¿Ï…Î­Ï„ ÏƒÎºÎ¿Ï": "silhouette score",
    "ÏƒÎºÎ¹Î¿ÏÎ½ÎµÏƒÏ‚": "skewness",
    "ÏƒÎ¿Ï…Ï€ÏŒÏÏ„": "support",
    "ÏƒÎ¿Ï†Ï„ ÎºÎ»Î¬ÏƒÏ„ÎµÏÎ¹Î½Î³Îº": "soft clustering",
    "ÏƒÎ¿Ï†Ï„Î¼Î±Î¾": "softmax",
    "ÏƒÏ€ÎµÏƒÎ¹Ï†Î¯ÏƒÎ¹Ï„Î¹": "specificity",
    "ÏƒÏ„Î±ÎºÎ¹Î½Î³Îº":"stacking",
    "ÏƒÏ„Î¬Ï„Ï‚ Î¼ÏŒÎ¿Ï…Î½Ï„ÎµÎ»Ï‚": "statsmodels",
    "ÏƒÏ„Î­Î¼Î¹Î½Î³Îº": "stemming",
    "ÏƒÏ„Î­Î½Ï„Î±ÏÎ½Ï„ Î½Ï„ÎµÎ²Î¹Î±Î¯Î¹ÏƒÎ¹Î¿Î½": "standard deviation",
    "ÏƒÏ„Î­ÏƒÎ¹Î¿Î½Î±ÏÎ¹Ï„Î¹": "stationarity",
    "ÏƒÏ„Î±Î½Ï„Î±ÏÎ½Ï„Î±ÏŠÎ¶Î­Î¹ÏƒÎ¹Î¿Î½": "standardization",
    "Ï„Î­Î½ÏƒÎ¿ÏÏ†Î»Î¿Î¿Ï…": "tensorflow",
    "Ï„Î±Î½Ï‡": "tanh",
    "Ï„ÎµÎ¾Ï„ ÎºÎ»Î¬ÏƒÎ¹Ï†Î¹ÎºÎ­Î¹ÏƒÎ¹Î¿Î½": "text classification",
    "Ï„ÎµÏƒÏ„ ÏƒÎµÏ„": "test set",
    "Ï„Î¶Î¹ Ï„Î¶Î¹ Ï€Î»Î¿Ï„ 2": "ggplot2",
    "Ï„Î¹ ÎµÏ‚ ÎµÎ½ Î¹": "tsne",
    "Ï„Î¹ ÎµÏ† Î¬Î¹ Î½Ï„Î¹ ÎµÏ†": "tf-idf",
    "Ï„Î¹ Ï„ÎµÏƒÏ„": "t-test",
    "Ï„Î¿ÎºÎµÎ½Î¹Î¶Î­Î¹ÏƒÎ¹Î¿Î½": "tokenization",
    "Ï„ÏÎ­Î¹Î½ Ï„ÎµÏƒÏ„ ÏƒÏ€Î»Î¹Ï„": "train-test split",
    "Ï„ÏÎ­Î¹Î½Î¹Î½Î³Îº ÏƒÎµÏ„": "training set",
    "Ï„ÏÎ±Î½ÏƒÏ†ÏŒÏÎ¼ÎµÏ": "transformer",
    "Ï„ÏÎµÎ½Ï„": "trend",
    "Ï„Ï…Ï‡Î±Î¯Î¿ Î´Î¬ÏƒÎ¿Ï‚": "random forest",
    "Ï†Î¯Ï„ÏƒÎ¿Ï…Ï ÎµÎ¾Ï„ÏÎ¬Î¾Î¹Î¿Î½": "feature extraction",
    "Ï†Î¯Ï„ÏƒÎ¿Ï…Ï Î¹Î¼Ï€ÏŒÏÏ„Î±Î½Ï‚": "feature importance",
    "Ï†Î¯Ï„ÏƒÎ¿Ï…Ï ÏƒÎµÎ»Î­Î¾Î¹Î¿Î½": "feature selection",
    "Ï†Î¯Ï„ÏƒÎ¿Ï…Ï ÏƒÎºÎ­Î¹Î»Î¹Î½Î³Îº": "feature scaling",
    "Ï†ÏÎ¯ÎºÎ¿Ï…ÎµÎ½Ï„ Î¬Î¹Ï„ÎµÎ¼ÏƒÎµÏ„": "frequent itemset",
    "Ï‡Î¬Î¹Ï€ÎµÏ Ï€Î±ÏÎ±Î¼Î¯Ï„ÎµÏ Ï„Î¿ÏÎ½Î¹Î½Î³Îº": "hyperparameter tuning",
    "Ï‡Î±ÏÎ½Ï„ ÎºÎ»Î¬ÏƒÏ„ÎµÏÎ¹Î½Î³Îº": "hard clustering",
    "Ï‡Î±ÏŠÎµÏÎ¬ÏÎ¹ÎºÎ±Î» ÎºÎ»Î¬ÏƒÏ„ÎµÏÎ¹Î½Î³Îº": "hierarchical clustering",
    "Ï‡Î±ÏŠÏ€ÏŒÎ¸ÎµÏƒÎ¹Ï‚ Ï„Î­ÏƒÏ„Î¹Î½Î³Îº": "hypothesis testing",
    "ÏŒÎ²ÎµÏÏ†Î¹Ï„Î¹Î½Î³Îº": "overfitting",
}

def correct_technical_terms(text):
    print(f"Refined text 1 : {text}")
    for greek, english in PHONETIC_GREEK_CORRECTIONS.items():
        text = text.replace(greek, english)
    print(f"Refined text 2 : {text}")
    return text


class AssistantEngine:
    def __init__(self):
        openai.api_key = config.OPENAI_API_KEY
        self.models = {}
        self.language_choice = config.DEFAULT_LANGUAGE
        self.question_counter = 0
        self.load_all_vosk_models()

    def load_all_vosk_models(self):
        for lang in config.AVAILABLE_LANGUAGES:
            model_subpath = config.LANGUAGE_MODEL_MAP.get(lang)
            model_path = os.path.join(config.MODEL_FOLDER, model_subpath)
            if not os.path.exists(model_path):
                print(f"âš ï¸ Model for {lang} not found at {model_path}")
                continue
            self.models[lang] = vosk.Model(model_path)
            print(f"âœ… Loaded Vosk model for {lang}")

    def recognize_audio(self, recorded_audio):
        model = self.models.get(self.language_choice)
        if not model:
            raise Exception(f"No model loaded for language {self.language_choice}")
        recognizer = vosk.KaldiRecognizer(model, 16000)
        recognizer.AcceptWaveform(recorded_audio)
        result = recognizer.Result()
        text = json.loads(result).get("text", "")
        return correct_technical_terms(text.strip())

    def refine_question(self, raw_text):
        try:
            protected_map = {}
            masked_text = raw_text
            # for i, term in enumerate(config.PROTECTED_TERMS):
            #     pattern = re.compile(re.escape(term), re.IGNORECASE)
            #     if pattern.search(masked_text):
            #         key = f"<TERM{i}>"
            #         protected_map[key] = term
            #         masked_text = pattern.sub(key, masked_text)

            refine_prompt = (
                f"You are {config.ROLE} in oral examination. Give a short answer in Greek"
            )


            client = openai.OpenAI(api_key=config.OPENAI_API_KEY)
            response = client.chat.completions.create(
                model=config.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": refine_prompt},
                    {"role": "user", "content": masked_text}
                ]
            )
            refined = response.choices[0].message.content.strip()

            for key, term in protected_map.items():
                refined = refined.replace(key, term)
            return refined
        except Exception as e:
            print(f"Error refining question: {e}")
            return raw_text

    def ask_ai(self, question):
        try:
            system_prompt = (f"You are a {config.ROLE}. Please type a suitable answer for oral examination . "
                             f"If possible in bullets to easy reading. "
                             f"Give a short Answer in simple greek ")
            client = openai.OpenAI(api_key=config.OPENAI_API_KEY)
            response = client.chat.completions.create(
                model=config.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error asking AI: {e}"

    def log_session(self, question, answer):
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(config.SESSION_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(f"\n[{timestamp}]\nQ: {question}\nA: {answer}\n" + "-"*50 + "\n")

    def next_question_number(self):
        self.question_counter += 1
        return self.question_counter
